<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Smith and Kutas 2015a | Single-Trial EEG Analysis</title>
  <meta name="description" content="3 Smith and Kutas 2015a | Single-Trial EEG Analysis" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Smith and Kutas 2015a | Single-Trial EEG Analysis" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Smith and Kutas 2015a | Single-Trial EEG Analysis" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fromer-et-al-2018.html">
<link rel="next" href="smith-and-kutas-2015b.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="general-stuff.html"><a href="general-stuff.html"><i class="fa fa-check"></i><b>1</b> General stuff</a><ul>
<li class="chapter" data-level="1.1" data-path="general-stuff.html"><a href="general-stuff.html#comparison-of-methods"><i class="fa fa-check"></i><b>1.1</b> Comparison of Methods</a></li>
<li class="chapter" data-level="1.2" data-path="general-stuff.html"><a href="general-stuff.html#determining-time-windows"><i class="fa fa-check"></i><b>1.2</b> Determining Time Windows</a></li>
<li class="chapter" data-level="1.3" data-path="general-stuff.html"><a href="general-stuff.html#limitations"><i class="fa fa-check"></i><b>1.3</b> Limitations</a></li>
<li class="chapter" data-level="1.4" data-path="general-stuff.html"><a href="general-stuff.html#conclusion"><i class="fa fa-check"></i><b>1.4</b> Conclusion</a></li>
<li class="chapter" data-level="1.5" data-path="general-stuff.html"><a href="general-stuff.html#to-do"><i class="fa fa-check"></i><b>1.5</b> TO DO</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html"><i class="fa fa-check"></i><b>2</b> Frömer et al. (2018)</a><ul>
<li class="chapter" data-level="2.1" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#cookbook"><i class="fa fa-check"></i><b>2.1</b> Cookbook</a></li>
<li class="chapter" data-level="2.2" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#behavioural-results"><i class="fa fa-check"></i><b>2.2</b> Behavioural Results</a></li>
<li class="chapter" data-level="2.3" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#brain-results-n2"><i class="fa fa-check"></i><b>2.3</b> Brain Results N2</a></li>
<li class="chapter" data-level="2.4" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#brain-results-p3b"><i class="fa fa-check"></i><b>2.4</b> Brain Results P3b</a></li>
<li class="chapter" data-level="2.5" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#results-brain-behaviour-relationship-accuracy"><i class="fa fa-check"></i><b>2.5</b> Results Brain Behaviour Relationship Accuracy</a></li>
<li class="chapter" data-level="2.6" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#results-brain-behaviour-relationship-rt"><i class="fa fa-check"></i><b>2.6</b> Results Brain Behaviour Relationship RT</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html"><i class="fa fa-check"></i><b>3</b> Smith and Kutas 2015a</a><ul>
<li class="chapter" data-level="3.1" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html#a-regression-based-framework-for-estimating-erps-rerp"><i class="fa fa-check"></i><b>3.1</b> A Regression-Based Framework for Estimating ERPs (rERP)</a></li>
<li class="chapter" data-level="3.2" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html#defining-predictors-for-rerp-analysis"><i class="fa fa-check"></i><b>3.2</b> Defining Predictors for rERP Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smith-and-kutas-2015b.html"><a href="smith-and-kutas-2015b.html"><i class="fa fa-check"></i><b>4</b> Smith and Kutas 2015b</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Single-Trial EEG Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="smith-and-kutas-2015a" class="section level1">
<h1><span class="header-section-number">3</span> Smith and Kutas 2015a</h1>
<p><a href="https://www.ncbi.nlm.nih.gov/pubmed/25141770">Smith</a> <span class="citation">(2015a)</span>. <strong>Regression-based estimation of ERP waveforms: I. The rERP framework.</strong></p>
<p>ERP averaging is an extraordinarily successful method, but can only be applied to a limited range of experimental designs. We introduce the regression-based rERP framework, which extends ERP averaging to handle arbitrary combinations of categorical and continuous covariates, partial confounding, non-linear effects, and overlapping responses to distinct events, all within a single unified system. rERPs enable a richer variety of paradigms (including high-N naturalistic designs) while preserving the advantages of traditional ERPs.</p>
<p>This article provides an accessible introduction to what rERPs are, why they are useful, how they are computed, and when we should expect them to be effective, particularly in cases of partial confounding. A companion article discusses how non-linear effects and overlap correction can be handled within this framework, as well as practical considerations around baselining, filtering, statistical testing, and artifact rejection. Free software implementing these techniques is available.</p>
<div id="a-regression-based-framework-for-estimating-erps-rerp" class="section level2">
<h2><span class="header-section-number">3.1</span> A Regression-Based Framework for Estimating ERPs (rERP)</h2>
<ul>
<li>Single method for estimating ERP waveforms.
<ul>
<li>Factorial or continuous designs or both.</li>
<li>Orthogonal or partially confounded design.</li>
<li>Linear or non-linear effects of continuous covariates.</li>
<li>Overlapping ERPs produced by events of interest or not.</li>
</ul></li>
<li>In the simplest case – a categorical design with no overlap correction – rERP estimates are mathematically identical to those produced by traditional averaging.</li>
</ul>
<div id="averaging-is-least-squares-regression" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Averaging is Least-Squares Regression</h3>
<p>Instead of trying to estimate the whole waveform at once, we start by working out how to estimate the value of the ERP at one single electrode and latency – e.g., 136 ms post-event at electrode Cz. (If we can do this, then we can estimate the rest of the ERP by just repeating our technique at every electrode and latency.) Estimate for every trial:</p>
<p><span class="math display">\[
y_i =\beta + \textrm{noise}_i
\]</span></p>
<ul>
<li>At every trial, the noise will be different, but the value of the ERP (at this latency and electrode) will be the same.</li>
<li>Usual suggestion to estimate <span class="math inline">\(\beta\)</span>: Take the average of <span class="math inline">\(y_i\)</span>.</li>
<li>If we knew the noise at every trial or the estimate for <span class="math inline">\(\beta\)</span>, we could solve the equations, but we don’t know neither.</li>
</ul>
<p>What we can use instead: The principle of <span style=" font-weight: bold;    color: #E8793A !important;">least squares</span>: We should choose our estimate of <span class="math inline">\(\beta\)</span> to be the number which makes our estimate for the total squared noise,</p>
<p><span class="math display">\[
\textrm{squared noise} =\sum_{i=1}^{n}\left(\textrm{noise }_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta\right)^{2}
\]</span></p>
<p>as small als possible. So we take the <span style=" font-weight: bold;    color: #E8793A !important;">derivate</span> of this formula:</p>
<p><span class="math display">\[
\frac{d}{d \beta} \textrm{ squared noise }=\frac{d}{d \beta} \sum_{i=1}^{n}\left(y_{i}-\beta\right)^{2}=\sum_{i=1}^{n}-2\left(y_{i}-\beta\right)
\]</span></p>
<p>And then set it equal to zero:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}-2\left(y_{i}-\beta\right)=0
\]</span></p>
<p>And solve for <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}-2\left(\sum_{i=1}^{n} y_{i}-\sum_{i=1}^{n} \beta\right) &amp;=0 \\ \sum_{i=1}^{n} y_{i} &amp;=\sum_{i=1}^{n} \beta \\ \sum_{i=1}^{n} y_{i} &amp;=n \beta \\ \frac{1}{n} \sum_{i=1}^{n} y_{i} &amp;=\beta \end{aligned}
\]</span></p>
<p><span style=" font-weight: bold;    color: #6BBFA6 !important;">Notice that this final formula turns out to be the standard formula for calculating the mean.</span> This means that according to the least-squares principle, the best way to estimate <span class="math inline">\(\beta\)</span> is to take the average of our measured values, <span class="math inline">\(y_i, ..., y_n\)</span>. This is the reason why using averaging to estimate ERPs makes sense in the first place.</p>
</div>
<div id="from-averaging-to-regression" class="section level3">
<h3><span class="header-section-number">3.1.2</span> From Averaging to Regression</h3>
<p>On the other hand, this formula …</p>
<p><span class="math display">\[
y_i =\beta + \textrm{noise}_i
\]</span></p>
<p>… is just a simple example of the general least-squares regression formula:</p>
<p><span class="math display">\[
y_{i}=\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\textrm{ noise }_{i}
\]</span></p>
<p>So, we see what happens when we estimate ERPs with the full regression formula instead of the simplified one. Again, <span class="math inline">\(y_i\)</span> values are set to the measured scalp potential at a single electrode, at a single latency, across different time-locked trials. The <span class="math inline">\(x_{ji}\)</span> values are the <span style=" font-weight: bold;    color: #6BBFA6 !important;">predictors</span> and indicate various properties of the stimulus presented on trial <span class="math inline">\(i\)</span>, coded numerically.</p>
<ul>
<li><p>The original ERP equation above effectively has a single predictor, <span class="math inline">\(x_{1i}\)</span>, whose value is always 1. We can think of this as a particularly vague property that simply indicates that there was an event.</p></li>
<li><p>We measure the <span class="math inline">\(y_i\)</span> values, specify the <span class="math inline">\(x_{ji}\)</span> values and then again use the principle of least-squares to find those values for <span class="math inline">\(\beta_1, \beta_2, ...\)</span> which together minimise the total squared noise.</p>
<ul>
<li>Each <span class="math inline">\(\beta\)</span> value then gives an estimate of some portion of the ERP at this electrode and latency.</li>
<li>Or, for any particular stimulus <span class="math inline">\(i\)</span>, we can compute the sum <span class="math inline">\(\beta_{1x_{1i}} + \beta_{2x_{2i}} + …\)</span>, which we call the model’s prediction of the ERP for a stimulus with these properties at this latency.</li>
</ul></li>
<li><p>Finding the <span class="math inline">\(\beta\)</span> values which satisfy the least-squares principle is somewhat more complicated than just taking the average, but not by much.</p>
<ul>
<li>Standard techniques that allow computers to accomplish this quickly and reliably.</li>
</ul></li>
<li><p>We then <span style=" font-weight: bold;    color: #6BBFA6 !important;">repeat these calculations</span> many times, once for <span style=" font-weight: bold;    color: #6BBFA6 !important;">each electrode and latency</span>.</p></li>
<li><p>Finally, we gather up all the computed <span class="math inline">\(\beta_1\)</span> values to make one waveform, all the <span class="math inline">\(\beta_2\)</span> values to make a second waveform, and so on for all of the <span class="math inline">\(\beta\)</span>s.</p>
<ul>
<li>The resulting waveforms can then be plotted, smoothed, entered into statistical analysis, have amplitude and latency measures extracted, and generally be treated exactly as if they were ERP waveform estimates obtained via averaging.</li>
</ul></li>
<li><p>Likewise, we can combine <span class="math inline">\(\beta\)</span>s together to compute the predicted waveforms for particular stimuli.</p>
<ul>
<li>These predictions can also be analyzed like ERP estimates obtained from averaging.</li>
</ul></li>
<li><p>To remind ourselves that our waveforms were estimated using regression instead of averaging, we call them rERPs.</p></li>
</ul>
</div>
</div>
<div id="defining-predictors-for-rerp-analysis" class="section level2">
<h2><span class="header-section-number">3.2</span> Defining Predictors for rERP Analysis</h2>
<div id="example-experiment" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Example Experiment</h3>
<p>A language comprehension experiment which created contexts in which participants had a graded expectation for either the word <em>a</em> or the word <em>an</em>, such as <em>The day was breezy so the boy went outside to fly (<strong>a</strong> kite/<strong>an</strong> airplane)</em>. Thus, our example design has one categorical covariate – word identity, a versus an – and one continuous covariate – word expectancy, which falls between 0 and 1.</p>
</div>
<div id="the-traditional-erp-as-an-intercept-term" class="section level3">
<h3><span class="header-section-number">3.2.2</span> The traditional ERP as an intercept term</h3>
<p>Suppose we define a single predictor: <span class="math inline">\(x_{1i} = 1\)</span></p>
<p>In linear regression terminology, this predictor is known as an intercept term. Then our regression equation is</p>
<p><span class="math display">\[
y_{i}=\beta_{1} x_{1 i}+\textrm{ noise }_{i}=\beta_{1}+\textrm{ noise }_{i}
\]</span></p>
<p>As above: When we find the least-squares solution, <span class="math inline">\(\beta_{1}\)</span> will end up equal to the mean of the <span class="math inline">\(y_i\)</span> values.</p>
<p><img src="pics/smith_a_fig1_1.jpg" width="1560" /></p>
<p>Results are identical to the averaging technique, but somewhat cumbersome to use, because if we have categorical factors with multiple levels, then it requires us to fit two different models – one on the <em>a</em> trials, and another on the <em>an</em> trials.</p>
</div>
<div id="multiple-erps-via-dummy-coding" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Multiple ERPs via dummy coding</h3>
<p>Instead of fitting multiple models, we can estimate both ERPs at once within a single regression model by using a trick known as dummy coding, which is one of the standard ways to handle categorical variables within regression models.</p>
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Note here:</span> “This is the default method of coding categorical variables used by SAS, and is also used by default by <code>R</code> for models that do not contain an intercept term.”
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">TO DO:</span> Look up models without an intercept term in <code>R</code> and what happens there?</li>
</ul></li>
</ul>
<p><span class="math display">\[
x_{1 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { if stimulus } i \text { is a}} \\ {0,} &amp; {\text { if stimulus } i \text { is an }}\end{array} \quad x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is a}} \\ {1,} &amp; {\text { if stimulus } i \text { is an }}\end{array}\right.\right.
\]</span></p>
<p><span style=" font-weight: bold;    color: #E8793A !important;">Note:</span> It said <span class="math inline">\(x_{1i}\)</span> for the second bracket, but that appears to be a mistake?</p>
<p>Plug this in the regression equation:</p>
<p><span class="math display">\[
y_{i}=\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\textrm{noise}_{i}
\]</span></p>
<p>then least-squares fitting will set <span class="math inline">\(\beta_{1}\)</span> to the average of the <em>a</em> trials, and <span class="math inline">\(\beta_{2}\)</span> to the average of the <em>an</em> trials.</p>
<p><img src="pics/smith_a_fig1_2.jpg" width="1560" /></p>
<p>It’s easy to see why this happens. If on trial <em>i</em> we displayed the word <em>a</em>, then <span class="math inline">\(x_{1i}\)</span> is 1 and <span class="math inline">\(x_{2i}\)</span> is 0, so we have</p>
<p><span class="math display">\[
y_{i}=\beta_{1} \times 1+\beta_{2} \times 0+\textrm{noise}_{i}=\beta_{1}+\textrm{noise}_{i}
\]</span></p>
<p>Or, if on trial <em>i</em> we displayed <em>an</em>, then <span class="math inline">\(x_{1i}\)</span> is 0 and <span class="math inline">\(x_{2i}\)</span> is 1, so we have</p>
<p><span class="math display">\[
y_{i}=\beta_{1} \times 0+\beta_{2} \times 1+\textrm{noise}_{i}=\beta_{2}+\textrm{noise}_{i}
\]</span></p>
<p>So effectively we end up <span style=" font-weight: bold;    color: #6BBFA6 !important;">fitting two copies of our previous intercept-only model</span>, on two different subsets of the data. The only difference from the previous example is that before, we did this by explicitly dividing our data into subsets; now, we just define our <span class="math inline">\(x\)</span>s and the appropriate data splitting happens automatically as a result of the least-squares fitting process. A generalized version of this “zero trick” can be used to combine arbitrary regression models into a single fit, and we’ll see later that this is useful for several different purposes.</p>
</div>
<div id="difference-erps-via-treament-coding" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Difference ERPs via treament coding</h3>
<p>More common approach to handle categorical variables in regressions: <span style=" font-weight: bold;    color: #E8793A !important;">treatment coding</span>, which means: <span style=" font-weight: bold;    color: #6BBFA6 !important;">dummy coding for all but one levels</span> of our factor (the level that is left out is called <span style=" font-weight: bold;    color: #6BBFA6 !important;">reference level</span>) and then adding an <span style=" font-weight: bold;    color: #6BBFA6 !important;">intercept term</span>.</p>
<p>With <em>a</em> stimulus as reference level:</p>
<p><span class="math display">\[
x_{1 i}=1, \quad x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \textrm{ is a}} \\ {1,} &amp; {\textrm{ if stimulus } i \textrm{ is an }}\end{array}\right.
\]</span></p>
<p>With this coding scheme, least-squares fitting will set <span class="math inline">\(\beta_{1}\)</span> to the average of the a trials (intercept for <em>a</em>!), and <span class="math inline">\(\beta_{2}\)</span> to the <span style=" font-weight: bold;    color: #6BBFA6 !important;">difference between the <em>an</em> trials and the <em>a</em> trials</span> (how much the intercept of <em>a</em> changes when the stimulus is set to <em>an</em>!), i.e., <span class="math inline">\(\beta_{2}\)</span> will be a conventional difference ERP.</p>
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Note here:</span> “This is the default method of coding categorical variables in <code>R</code> and SPSS.”
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Data coding in R:</span> <strong>This</strong> is the reason why coding e.g. condition a = 1 and condition b = 0 will model the intercept for condition b and <span class="math inline">\(\beta\)</span> will be how much condition b differs from the intercept of condition a. And coding it as a = -1 and b = 1 will result in the average intercept for both conditions and <span class="math inline">\(\beta\)</span> will be how much both conditions differ from the average (overall) intercept.</li>
</ul></li>
</ul>
<p><img src="pics/smith_a_fig1_3.jpg" width="1560" /></p>
<p>This is why it’s called <span style=" font-weight: bold;    color: #E8793A !important;">treatment coding</span>: If the control condition is the reference level and you apply various “treatments” on top of it, then the resulting <span class="math inline">\(\beta\)</span> show you how these “treatments” change the ERP response versus the control.</p>
<p><span style=" font-weight: bold;    color: #E8793A !important;">Important:</span> The fitted value of <span class="math inline">\(\beta_j\)</span> does not only depend on how the <span class="math inline">\(j\)</span>th predictor is defined, but on how <span style=" font-weight: bold;    color: #6BBFA6 !important;">all</span> the predictors <span class="math inline">\(x_{1i}, ..., x_{ni}\)</span> are defined. From example b) to example c), we only changed the definition of <span class="math inline">\(x_{1i}\)</span>, but it was the interpretation of <span class="math inline">\(\beta_2\)</span> that changed.</p>
<p>“The least-squares fitting process does not care about the <span class="math inline">\(\beta\)</span> values directly. It only cares about the predicted values (the rightmost column in the Figures). It will pick whichever <span class="math inline">\(\beta\)</span> values make these predictions match the data as closely as possible. Since the predictions are created by combining multiple <span class="math inline">\(\beta\)</span> values together, this means that the chosen <span class="math inline">\(\beta\)</span> values are not the ones that individually match the data best, but the ones that are most effective at working together. In the treatment coding case, <span class="math inline">\(\beta_1\)</span> must work alone to match the a stimuli, while for the an stimuli, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> work together; so the most effective teamwork is achieved when <span class="math inline">\(\beta_1\)</span> focuses on matching the a stimuli while <span class="math inline">\(\beta_2\)</span> focuses on correcting <span class="math inline">\(\beta_1\)</span> so that their combination will match the an stimuli.”</p>
<p>But it also creates a <span style=" font-weight: bold;    color: #6BBFA6 !important;">potential problem</span> for some coding schemes that might otherwise seem reasonable: those in which <span style=" font-weight: bold;    color: #6BBFA6 !important;">several predictors are perfectly collinear</span>, i.e., redundant.
- If you accidentally enter the same predictor twice.
- If you leave some cell out of the design</p>
<p>Or – less obviously – if you use the original simple dummy coding scheme, but for two different factors at once. For instance, pretend that sometimes in this experiment the critical items were displayed in UPPERCASE. Then we could define four dummy-coded predictors:</p>
<p><span class="math display">\[
x_{1 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { if stimulus } i \text { is a}} \\ {0,} &amp; {\text { if stimulus } i \text { is an }}\end{array} \quad x_{2 i}=\left\{\begin{array}{l}{0, \text { if stimulus } i \text { is a}} \\ {1, \text { if stimulus } i \text { is an }}\end{array}\right.\right.
\]</span></p>
<p><span class="math display">\[
x_{3 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { if stimulus } i \text { is lowercase }} \\ {0,} &amp; {\text { if stimulus } i \text { is UPPERCASE }}\end{array} \quad x_{4 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is lowercase }} \\ {1,} &amp; {\text { if stimulus } i \text { is UPPERCASE }}\end{array}\right.\right.
\]</span></p>
<p>“Suppose that there is some component, say the N1, which is identical between all four conditions. Then one way the <span class="math inline">\(\beta\)</span>s could work together to capture this effect would be to say that a and an are both associated with an N1, and put the N1’s deflection into the <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> waveforms. Another way to explain it would be to say that the N1 is triggered by both uppercase and lowercase words, and let the <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(\beta_4\)</span> waveforms take care of it. Or maybe a and an both trigger a positivity during this window, but uppercase-ness and lowercase-ness both generate an even greater negativity which cancels it out – that would also be consistent with the data. Because all these combinations of <span class="math inline">\(\beta\)</span> values ultimately lead to the same predictions, least-squares fitting has no way to choose among them. Some regression software, when confronted with this situation, will respond by silently and semi-arbitrarily picking one of the equivalent and equally-best combinations of <span class="math inline">\(\beta\)</span> values.”</p>
<p>Fortunately, treatment coding always allows categorical variables and their interactions to be straightforwardly coded in a non-redundant way. For example, applying treatment coding to our uppercase/lowercase analysis gives a non-redundant set of predictors:</p>
<p><span class="math display">\[
x_{1 i}=1 \quad x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is a}} \\ {1,} &amp; {\text { if stimulus } i \text { is an }}\end{array}\right.
\]</span></p>
<p><span class="math display">\[
x_{3 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is lowercase }} \\ {1,} &amp; {\text { if stimulus } i \text { is UPPERCASE }}\end{array} \quad x_{4 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is either lowercase or a }} \\ {1,} &amp; {\text { if stimulus } i \text { is an UPPERCASE an }}\end{array}\right.\right.
\]</span></p>
<p>If included, <span class="math inline">\(x_{4i} = x_{2i} \times x_{3i}\)</span> - the interaction. Here <span class="math inline">\(\beta_1\)</span> will estimate the ERP for lowercase a, <span class="math inline">\(\beta_2\)</span> the difference between <strong>lowercase</strong> <em>a</em> and <strong>lowercase</strong> <em>an</em>, <span class="math inline">\(\beta_3\)</span> the difference between <strong>lowercase</strong> <em>a</em> and <strong>uppercase</strong> <em>A</em>, and <span class="math inline">\(\beta_4\)</span> will be a <span style=" font-weight: bold;    color: #E8793A !important;">difference-of-differences</span> ERP: if we first calculated the difference ERP between <strong>uppercase</strong> <em>AN</em> and <strong>uppercase</strong> <em>A</em>, and then calculated the difference ERP between <strong>lowercase</strong> <em>an</em> and <strong>lowercase</strong> <em>a</em>, then <span class="math inline">\(\beta_4\)</span> will be the difference between these two difference ERPs. Notice that this means <span class="math inline">\(\beta_4\)</span> will be significantly different from zero exactly when there is a <span style=" font-weight: bold;    color: #6BBFA6 !important;">non-additive interaction</span> between our two factors.</p>
<p>In general there are many <span style=" font-weight: bold;    color: #E8793A !important;">different but equivalent ways</span> to represent any given linear model; a simple example is shown in Fig. b) versus Fig. c). <span style=" font-weight: bold;    color: #E8793A !important;">Non-redundant codings</span> give us the power to choose the representation we find most interpretable.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fromer-et-al-2018.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="smith-and-kutas-2015b.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": 2,
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
