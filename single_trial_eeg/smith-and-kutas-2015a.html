<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Smith and Kutas 2015a | Single-Trial EEG Analysis</title>
  <meta name="description" content="3 Smith and Kutas 2015a | Single-Trial EEG Analysis" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Smith and Kutas 2015a | Single-Trial EEG Analysis" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Smith and Kutas 2015a | Single-Trial EEG Analysis" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fromer-et-al-2018.html">
<link rel="next" href="smith-and-kutas-2015b.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="general-stuff.html"><a href="general-stuff.html"><i class="fa fa-check"></i><b>1</b> General stuff</a><ul>
<li class="chapter" data-level="1.1" data-path="general-stuff.html"><a href="general-stuff.html#comparison-of-methods"><i class="fa fa-check"></i><b>1.1</b> Comparison of Methods</a></li>
<li class="chapter" data-level="1.2" data-path="general-stuff.html"><a href="general-stuff.html#determining-time-windows"><i class="fa fa-check"></i><b>1.2</b> Determining Time Windows</a></li>
<li class="chapter" data-level="1.3" data-path="general-stuff.html"><a href="general-stuff.html#limitations"><i class="fa fa-check"></i><b>1.3</b> Limitations</a></li>
<li class="chapter" data-level="1.4" data-path="general-stuff.html"><a href="general-stuff.html#conclusion"><i class="fa fa-check"></i><b>1.4</b> Conclusion</a></li>
<li class="chapter" data-level="1.5" data-path="general-stuff.html"><a href="general-stuff.html#to-do"><i class="fa fa-check"></i><b>1.5</b> TO DO</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html"><i class="fa fa-check"></i><b>2</b> Frömer et al. (2018)</a><ul>
<li class="chapter" data-level="2.1" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#cookbook"><i class="fa fa-check"></i><b>2.1</b> Cookbook</a></li>
<li class="chapter" data-level="2.2" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#behavioural-results"><i class="fa fa-check"></i><b>2.2</b> Behavioural Results</a></li>
<li class="chapter" data-level="2.3" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#brain-results-n2"><i class="fa fa-check"></i><b>2.3</b> Brain Results N2</a></li>
<li class="chapter" data-level="2.4" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#brain-results-p3b"><i class="fa fa-check"></i><b>2.4</b> Brain Results P3b</a></li>
<li class="chapter" data-level="2.5" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#results-brain-behaviour-relationship-accuracy"><i class="fa fa-check"></i><b>2.5</b> Results Brain Behaviour Relationship Accuracy</a></li>
<li class="chapter" data-level="2.6" data-path="fromer-et-al-2018.html"><a href="fromer-et-al-2018.html#results-brain-behaviour-relationship-rt"><i class="fa fa-check"></i><b>2.6</b> Results Brain Behaviour Relationship RT</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html"><i class="fa fa-check"></i><b>3</b> Smith and Kutas 2015a</a><ul>
<li class="chapter" data-level="3.1" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html#a-regression-based-framework-for-estimating-erps-rerp"><i class="fa fa-check"></i><b>3.1</b> A Regression-Based Framework for Estimating ERPs (rERP)</a></li>
<li class="chapter" data-level="3.2" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html#defining-predictors-for-rerp-analysis"><i class="fa fa-check"></i><b>3.2</b> Defining Predictors for rERP Analysis</a></li>
<li class="chapter" data-level="3.3" data-path="smith-and-kutas-2015a.html"><a href="smith-and-kutas-2015a.html#collinearity"><i class="fa fa-check"></i><b>3.3</b> Collinearity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smith-and-kutas-2015b.html"><a href="smith-and-kutas-2015b.html"><i class="fa fa-check"></i><b>4</b> Smith and Kutas 2015b</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Single-Trial EEG Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="smith-and-kutas-2015a" class="section level1">
<h1><span class="header-section-number">3</span> Smith and Kutas 2015a</h1>
<p><a href="https://www.ncbi.nlm.nih.gov/pubmed/25141770">Smith</a> <span class="citation">(2015a)</span>. <strong>Regression-based estimation of ERP waveforms: I. The rERP framework.</strong></p>
<p>ERP averaging is an extraordinarily successful method, but can only be applied to a limited range of experimental designs. We introduce the regression-based rERP framework, which extends ERP averaging to handle arbitrary combinations of categorical and continuous covariates, partial confounding, non-linear effects, and overlapping responses to distinct events, all within a single unified system. rERPs enable a richer variety of paradigms (including high-N naturalistic designs) while preserving the advantages of traditional ERPs.</p>
<p>This article provides an accessible introduction to what rERPs are, why they are useful, how they are computed, and when we should expect them to be effective, particularly in cases of partial confounding. A companion article discusses how non-linear effects and overlap correction can be handled within this framework, as well as practical considerations around baselining, filtering, statistical testing, and artifact rejection. Free software implementing these techniques is available.</p>
<div id="a-regression-based-framework-for-estimating-erps-rerp" class="section level2">
<h2><span class="header-section-number">3.1</span> A Regression-Based Framework for Estimating ERPs (rERP)</h2>
<ul>
<li>Single method for estimating ERP waveforms.
<ul>
<li>Factorial or continuous designs or both.</li>
<li>Orthogonal or partially confounded design.</li>
<li>Linear or non-linear effects of continuous covariates.</li>
<li>Overlapping ERPs produced by events of interest or not.</li>
</ul></li>
<li>In the simplest case – a categorical design with no overlap correction – rERP estimates are mathematically identical to those produced by traditional averaging.</li>
</ul>
<div id="averaging-is-least-squares-regression" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Averaging is Least-Squares Regression</h3>
<p>Instead of trying to estimate the whole waveform at once, we start by working out how to estimate the value of the ERP at one single electrode and latency – e.g., 136 ms post-event at electrode Cz. (If we can do this, then we can estimate the rest of the ERP by just repeating our technique at every electrode and latency.) Estimate for every trial:</p>
<p><span class="math display">\[
y_i =\beta + \textrm{noise}_i
\]</span></p>
<ul>
<li>At every trial, the noise will be different, but the value of the ERP (at this latency and electrode) will be the same.</li>
<li>Usual suggestion to estimate <span class="math inline">\(\beta\)</span>: Take the average of <span class="math inline">\(y_i\)</span>.</li>
<li>If we knew the noise at every trial or the estimate for <span class="math inline">\(\beta\)</span>, we could solve the equations, but we don’t know neither.</li>
</ul>
<p>What we can use instead: The principle of <span style=" font-weight: bold;    color: #E8793A !important;">least squares</span>: We should choose our estimate of <span class="math inline">\(\beta\)</span> to be the number which makes our estimate for the total squared noise,</p>
<p><span class="math display">\[
\textrm{squared noise} =\sum_{i=1}^{n}\left(\textrm{noise }_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta\right)^{2}
\]</span></p>
<p>as small als possible. So we take the <span style=" font-weight: bold;    color: #E8793A !important;">derivate</span> of this formula:</p>
<p><span class="math display">\[
\frac{d}{d \beta} \textrm{ squared noise }=\frac{d}{d \beta} \sum_{i=1}^{n}\left(y_{i}-\beta\right)^{2}=\sum_{i=1}^{n}-2\left(y_{i}-\beta\right)
\]</span></p>
<p>And then set it equal to zero:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}-2\left(y_{i}-\beta\right)=0
\]</span></p>
<p>And solve for <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}-2\left(\sum_{i=1}^{n} y_{i}-\sum_{i=1}^{n} \beta\right) &amp;=0 \\ \sum_{i=1}^{n} y_{i} &amp;=\sum_{i=1}^{n} \beta \\ \sum_{i=1}^{n} y_{i} &amp;=n \beta \\ \frac{1}{n} \sum_{i=1}^{n} y_{i} &amp;=\beta \end{aligned}
\]</span></p>
<p><span style=" font-weight: bold;    color: #6BBFA6 !important;">Notice that this final formula turns out to be the standard formula for calculating the mean.</span> This means that according to the least-squares principle, the best way to estimate <span class="math inline">\(\beta\)</span> is to take the average of our measured values, <span class="math inline">\(y_i, ..., y_n\)</span>. This is the reason why using averaging to estimate ERPs makes sense in the first place.</p>
</div>
<div id="from-averaging-to-regression" class="section level3">
<h3><span class="header-section-number">3.1.2</span> From Averaging to Regression</h3>
<p>On the other hand, this formula …</p>
<p><span class="math display">\[
y_i =\beta + \textrm{noise}_i
\]</span></p>
<p>… is just a simple example of the general least-squares regression formula:</p>
<p><span class="math display">\[
y_{i}=\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\textrm{ noise }_{i}
\]</span></p>
<p>So, we see what happens when we estimate ERPs with the full regression formula instead of the simplified one. Again, <span class="math inline">\(y_i\)</span> values are set to the measured scalp potential at a single electrode, at a single latency, across different time-locked trials. The <span class="math inline">\(x_{ji}\)</span> values are the <span style=" font-weight: bold;    color: #6BBFA6 !important;">predictors</span> and indicate various properties of the stimulus presented on trial <span class="math inline">\(i\)</span>, coded numerically.</p>
<ul>
<li><p>The original ERP equation above effectively has a single predictor, <span class="math inline">\(x_{1i}\)</span>, whose value is always 1. We can think of this as a particularly vague property that simply indicates that there was an event.</p></li>
<li><p>We measure the <span class="math inline">\(y_i\)</span> values, specify the <span class="math inline">\(x_{ji}\)</span> values and then again use the principle of least-squares to find those values for <span class="math inline">\(\beta_1, \beta_2, ...\)</span> which together minimise the total squared noise.</p>
<ul>
<li>Each <span class="math inline">\(\beta\)</span> value then gives an estimate of some portion of the ERP at this electrode and latency.</li>
<li>Or, for any particular stimulus <span class="math inline">\(i\)</span>, we can compute the sum <span class="math inline">\(\beta_{1x_{1i}} + \beta_{2x_{2i}} + …\)</span>, which we call the model’s prediction of the ERP for a stimulus with these properties at this latency.</li>
</ul></li>
<li><p>Finding the <span class="math inline">\(\beta\)</span> values which satisfy the least-squares principle is somewhat more complicated than just taking the average, but not by much.</p>
<ul>
<li>Standard techniques that allow computers to accomplish this quickly and reliably.</li>
</ul></li>
<li><p>We then <span style=" font-weight: bold;    color: #6BBFA6 !important;">repeat these calculations</span> many times, once for <span style=" font-weight: bold;    color: #6BBFA6 !important;">each electrode and latency</span>.</p></li>
<li><p>Finally, we gather up all the computed <span class="math inline">\(\beta_1\)</span> values to make one waveform, all the <span class="math inline">\(\beta_2\)</span> values to make a second waveform, and so on for all of the <span class="math inline">\(\beta\)</span>s.</p>
<ul>
<li>The resulting waveforms can then be plotted, smoothed, entered into statistical analysis, have amplitude and latency measures extracted, and generally be treated exactly as if they were ERP waveform estimates obtained via averaging.</li>
</ul></li>
<li><p>Likewise, we can combine <span class="math inline">\(\beta\)</span>s together to compute the predicted waveforms for particular stimuli.</p>
<ul>
<li>These predictions can also be analyzed like ERP estimates obtained from averaging.</li>
</ul></li>
<li><p>To remind ourselves that our waveforms were estimated using regression instead of averaging, we call them rERPs.</p></li>
</ul>
</div>
</div>
<div id="defining-predictors-for-rerp-analysis" class="section level2">
<h2><span class="header-section-number">3.2</span> Defining Predictors for rERP Analysis</h2>
<div id="example-experiment" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Example Experiment</h3>
<p>A language comprehension experiment which created contexts in which participants had a graded expectation for either the word <em>a</em> or the word <em>an</em>, such as <em>The day was breezy so the boy went outside to fly (<strong>a</strong> kite/<strong>an</strong> airplane)</em>. Thus, our example design has one categorical covariate – word identity, a versus an – and one continuous covariate – word expectancy, which falls between 0 and 1.</p>
</div>
<div id="the-traditional-erp-as-an-intercept-term" class="section level3">
<h3><span class="header-section-number">3.2.2</span> The traditional ERP as an intercept term</h3>
<p>Suppose we define a single predictor: <span class="math inline">\(x_{1i} = 1\)</span></p>
<p>In linear regression terminology, this predictor is known as an intercept term. Then our regression equation is</p>
<p><span class="math display">\[
y_{i}=\beta_{1} x_{1 i}+\textrm{ noise }_{i}=\beta_{1}+\textrm{ noise }_{i}
\]</span></p>
<p>As above: When we find the least-squares solution, <span class="math inline">\(\beta_{1}\)</span> will end up equal to the mean of the <span class="math inline">\(y_i\)</span> values.</p>
<div class="figure"><span id="fig:unnamed-chunk-15"></span>
<img src="pics/smith_a_fig1_1.jpg" alt="Separate analysis for a and an trials." width="1560" />
<p class="caption">
Figure 3.1: Separate analysis for a and an trials.
</p>
</div>
<p>Results are identical to the averaging technique, but somewhat cumbersome to use, because if we have categorical factors with multiple levels, then it requires us to fit two different models – one on the <em>a</em> trials, and another on the <em>an</em> trials.</p>
</div>
<div id="multiple-erps-via-dummy-coding" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Multiple ERPs via dummy coding</h3>
<p>Instead of fitting multiple models, we can estimate both ERPs at once within a single regression model by using a trick known as dummy coding, which is one of the standard ways to handle categorical variables within regression models.</p>
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Note here:</span> “This is the default method of coding categorical variables used by SAS, and is also used by default by <code>R</code> for models that do not contain an intercept term.”
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">TO DO:</span> Look up models without an intercept term in <code>R</code> and what happens there?</li>
</ul></li>
</ul>
<p><span class="math display">\[
x_{1 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { if stimulus } i \text { is a}} \\ {0,} &amp; {\text { if stimulus } i \text { is an }}\end{array} \quad x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is a}} \\ {1,} &amp; {\text { if stimulus } i \text { is an }}\end{array}\right.\right.
\]</span></p>
<p><span style=" font-weight: bold;    color: #E8793A !important;">Note:</span> It said <span class="math inline">\(x_{1i}\)</span> for the second bracket, but that appears to be a mistake?</p>
<p>Plug this in the regression equation:</p>
<p><span class="math display">\[
y_{i}=\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\textrm{noise}_{i}
\]</span></p>
<p>then least-squares fitting will set <span class="math inline">\(\beta_{1}\)</span> to the average of the <em>a</em> trials, and <span class="math inline">\(\beta_{2}\)</span> to the average of the <em>an</em> trials.</p>
<div class="figure"><span id="fig:unnamed-chunk-16"></span>
<img src="pics/smith_a_fig1_2.jpg" alt="Dummy coding to combine both models for a and an trials into a single model." width="1560" />
<p class="caption">
Figure 3.2: Dummy coding to combine both models for a and an trials into a single model.
</p>
</div>
<p>It’s easy to see why this happens. If on trial <em>i</em> we displayed the word <em>a</em>, then <span class="math inline">\(x_{1i}\)</span> is 1 and <span class="math inline">\(x_{2i}\)</span> is 0, so we have</p>
<p><span class="math display">\[
y_{i}=\beta_{1} \times 1+\beta_{2} \times 0+\textrm{noise}_{i}=\beta_{1}+\textrm{noise}_{i}
\]</span></p>
<p>Or, if on trial <em>i</em> we displayed <em>an</em>, then <span class="math inline">\(x_{1i}\)</span> is 0 and <span class="math inline">\(x_{2i}\)</span> is 1, so we have</p>
<p><span class="math display">\[
y_{i}=\beta_{1} \times 0+\beta_{2} \times 1+\textrm{noise}_{i}=\beta_{2}+\textrm{noise}_{i}
\]</span></p>
<p>So effectively we end up <span style=" font-weight: bold;    color: #6BBFA6 !important;">fitting two copies of our previous intercept-only model</span>, on two different subsets of the data. The only difference from the previous example is that before, we did this by explicitly dividing our data into subsets; now, we just define our <span class="math inline">\(x\)</span>s and the appropriate data splitting happens automatically as a result of the least-squares fitting process. A generalized version of this “zero trick” can be used to combine arbitrary regression models into a single fit, and we’ll see later that this is useful for several different purposes.</p>
</div>
<div id="difference-erps-via-treament-coding" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Difference ERPs via treament coding</h3>
<p>More common approach to handle categorical variables in regressions: <span style=" font-weight: bold;    color: #E8793A !important;">treatment coding</span>, which means: <span style=" font-weight: bold;    color: #6BBFA6 !important;">dummy coding for all but one levels</span> of our factor (the level that is left out is called <span style=" font-weight: bold;    color: #6BBFA6 !important;">reference level</span>) and then adding an <span style=" font-weight: bold;    color: #6BBFA6 !important;">intercept term</span>.</p>
<p>With <em>a</em> stimulus as reference level:</p>
<p><span class="math display">\[
x_{1 i}=1, \quad x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \textrm{ is a}} \\ {1,} &amp; {\textrm{ if stimulus } i \textrm{ is an }}\end{array}\right.
\]</span></p>
<p>With this coding scheme, least-squares fitting will set <span class="math inline">\(\beta_{1}\)</span> to the average of the a trials (intercept for <em>a</em>!), and <span class="math inline">\(\beta_{2}\)</span> to the <span style=" font-weight: bold;    color: #6BBFA6 !important;">difference between the <em>an</em> trials and the <em>a</em> trials</span> (how much the intercept of <em>a</em> changes when the stimulus is set to <em>an</em>!), i.e., <span class="math inline">\(\beta_{2}\)</span> will be a conventional difference ERP.</p>
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Note here:</span> “This is the default method of coding categorical variables in <code>R</code> and SPSS.”
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Data coding in R:</span> <strong>This</strong> is the reason why coding e.g. condition a = 1 and condition b = 0 will model the intercept for condition b and <span class="math inline">\(\beta\)</span> will be how much condition b differs from the intercept of condition a. And coding it as a = -1 and b = 1 will result in the average intercept for both conditions and <span class="math inline">\(\beta\)</span> will be how much both conditions differ from the average (overall) intercept.</li>
</ul></li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-17"></span>
<img src="pics/smith_a_fig1_3.jpg" alt="Treatment coding to combine both models for a and an trials into a single model." width="1560" />
<p class="caption">
Figure 3.3: Treatment coding to combine both models for a and an trials into a single model.
</p>
</div>
<p>This is why it’s called <span style=" font-weight: bold;    color: #E8793A !important;">treatment coding</span>: If the control condition is the reference level and you apply various “treatments” on top of it, then the resulting <span class="math inline">\(\beta\)</span> show you how these “treatments” change the ERP response versus the control.</p>
<p><span style=" font-weight: bold;    color: #E8793A !important;">Important:</span> The fitted value of <span class="math inline">\(\beta_j\)</span> does not only depend on how the <span class="math inline">\(j\)</span>th predictor is defined, but on how <span style=" font-weight: bold;    color: #6BBFA6 !important;">all</span> the predictors <span class="math inline">\(x_{1i}, ..., x_{ni}\)</span> are defined. From Fig. 3.2 to Fig. 3.3, we only changed the definition of <span class="math inline">\(x_{1i}\)</span>, but it was the interpretation of <span class="math inline">\(\beta_2\)</span> that changed.</p>
<p>“The least-squares fitting process does not care about the <span class="math inline">\(\beta\)</span> values directly. It only cares about the predicted values (the rightmost column in the Figures). It will pick whichever <span class="math inline">\(\beta\)</span> values make these predictions match the data as closely as possible. Since the predictions are created by combining multiple <span class="math inline">\(\beta\)</span> values together, this means that the chosen <span class="math inline">\(\beta\)</span> values are not the ones that individually match the data best, but the ones that are most effective at working together. In the treatment coding case, <span class="math inline">\(\beta_1\)</span> must work alone to match the a stimuli, while for the an stimuli, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> work together; so the most effective teamwork is achieved when <span class="math inline">\(\beta_1\)</span> focuses on matching the a stimuli while <span class="math inline">\(\beta_2\)</span> focuses on correcting <span class="math inline">\(\beta_1\)</span> so that their combination will match the an stimuli.”</p>
<p>But it also creates a <span style=" font-weight: bold;    color: #6BBFA6 !important;">potential problem</span> for some coding schemes that might otherwise seem reasonable: those in which <span style=" font-weight: bold;    color: #6BBFA6 !important;">several predictors are perfectly collinear</span>, i.e., redundant.
- If you accidentally enter the same predictor twice.
- If you leave some cell out of the design</p>
<p>Or – less obviously – if you use the original simple dummy coding scheme, but for two different factors at once. For instance, pretend that sometimes in this experiment the critical items were displayed in UPPERCASE. Then we could define four dummy-coded predictors:</p>
<p><span class="math display">\[
x_{1 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { if stimulus } i \text { is a}} \\ {0,} &amp; {\text { if stimulus } i \text { is an }}\end{array} \quad x_{2 i}=\left\{\begin{array}{l}{0, \text { if stimulus } i \text { is a}} \\ {1, \text { if stimulus } i \text { is an }}\end{array}\right.\right.
\]</span></p>
<p><span class="math display">\[
x_{3 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { if stimulus } i \text { is lowercase }} \\ {0,} &amp; {\text { if stimulus } i \text { is UPPERCASE }}\end{array} \quad x_{4 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is lowercase }} \\ {1,} &amp; {\text { if stimulus } i \text { is UPPERCASE }}\end{array}\right.\right.
\]</span></p>
<p>“Suppose that there is some component, say the N1, which is identical between all four conditions. Then one way the <span class="math inline">\(\beta\)</span>s could work together to capture this effect would be to say that a and an are both associated with an N1, and put the N1’s deflection into the <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> waveforms. Another way to explain it would be to say that the N1 is triggered by both uppercase and lowercase words, and let the <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(\beta_4\)</span> waveforms take care of it. Or maybe a and an both trigger a positivity during this window, but uppercase-ness and lowercase-ness both generate an even greater negativity which cancels it out – that would also be consistent with the data. Because all these combinations of <span class="math inline">\(\beta\)</span> values ultimately lead to the same predictions, least-squares fitting has no way to choose among them. Some regression software, when confronted with this situation, will respond by silently and semi-arbitrarily picking one of the equivalent and equally-best combinations of <span class="math inline">\(\beta\)</span> values.”</p>
<p>Fortunately, treatment coding always allows categorical variables and their interactions to be straightforwardly coded in a non-redundant way. For example, applying treatment coding to our uppercase/lowercase analysis gives a non-redundant set of predictors:</p>
<p><span class="math display">\[
x_{1 i}=1 \quad x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is a}} \\ {1,} &amp; {\text { if stimulus } i \text { is an }}\end{array}\right.
\]</span></p>
<p><span class="math display">\[
x_{3 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is lowercase }} \\ {1,} &amp; {\text { if stimulus } i \text { is UPPERCASE }}\end{array} \quad x_{4 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if stimulus } i \text { is either lowercase or a }} \\ {1,} &amp; {\text { if stimulus } i \text { is an UPPERCASE an }}\end{array}\right.\right.
\]</span></p>
<p>If included, <span class="math inline">\(x_{4i} = x_{2i} \times x_{3i}\)</span> - the interaction. Here <span class="math inline">\(\beta_1\)</span> will estimate the ERP for lowercase a, <span class="math inline">\(\beta_2\)</span> the difference between <strong>lowercase</strong> <em>a</em> and <strong>lowercase</strong> <em>an</em>, <span class="math inline">\(\beta_3\)</span> the difference between <strong>lowercase</strong> <em>a</em> and <strong>uppercase</strong> <em>A</em>, and <span class="math inline">\(\beta_4\)</span> will be a <span style=" font-weight: bold;    color: #E8793A !important;">difference-of-differences</span> ERP: if we first calculated the difference ERP between <strong>uppercase</strong> <em>AN</em> and <strong>uppercase</strong> <em>A</em>, and then calculated the difference ERP between <strong>lowercase</strong> <em>an</em> and <strong>lowercase</strong> <em>a</em>, then <span class="math inline">\(\beta_4\)</span> will be the difference between these two difference ERPs. Notice that this means <span class="math inline">\(\beta_4\)</span> will be significantly different from zero exactly when there is a <span style=" font-weight: bold;    color: #6BBFA6 !important;">non-additive interaction</span> between our two factors.</p>
<p>In general there are many <span style=" font-weight: bold;    color: #E8793A !important;">different but equivalent ways</span> to represent any given linear model; a simple example is shown in Fig. 3.2 versus Fig. 3.3. <span style=" font-weight: bold;    color: #E8793A !important;">Non-redundant codings</span> give us the power to choose the representation we find most interpretable.</p>
</div>
<div id="slope-erps-from-numeric-predictors" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Slope ERPs from numeric predictors</h3>
<ul>
<li>So far: Everything could have also been done with averaging/computation of differences.</li>
<li>Now, we include <span style=" font-weight: bold;    color: #E8793A !important;">continuous covariates</span>
<ul>
<li>Here: <span style=" font-weight: bold;    color: #E8793A !important;">Word expectancy</span> from 0 - 1.</li>
<li>Expactancy for each word defined in normation task before (<a href="https://en.wikipedia.org/wiki/Cloze_test">cloze test</a>).</li>
</ul></li>
<li>“In practice, we’ll always include an intercept term as well.”
<ul>
<li>First: Note how they assumed that it was clear we talked about something different than an intercept here.</li>
<li>Usually necessary because otherwise, we’d assume that our outcome is zero when our predictor is zero, which is unreasonable most of the times.</li>
</ul></li>
</ul>
<p>Definition of predictors for the word expectancy effect:</p>
<p><span class="math display">\[
\boldsymbol{x}_{1 i}=\mathbf{1}, \quad x_{2 i}=\textrm{ word expectancy on trial } i
\]</span></p>
<ul>
<li>Now <span class="math inline">\(\beta_1\)</span> will give a kind of <span style=" font-weight: bold;    color: #E8793A !important;">baseline</span> ERP — the brain signal we expect to see in reaction to items with <span class="math inline">\(x_{2i} = 0\)</span>, i.e., items which were never guessed in the norming task.</li>
<li><span class="math inline">\(\beta_2\)</span> estimates how much this ERP <span style=" font-weight: bold;    color: #6BBFA6 !important;">changes with each unit change</span> in expectancy.
<ul>
<li>Going from expectancy 0 to expectancy 0.5 will produce a change of <span class="math inline">\(0.5 \times \beta_2\)</span> in the predicted ERP.</li>
<li><span class="math inline">\(\beta_2\)</span> is the <span style=" font-weight: bold;    color: #E8793A !important;">slope of the regression line</span> that relates <span class="math inline">\(x_{2i}\)</span> and <span class="math inline">\(y_i\)</span>, so we refer to this as a slope rERP.</li>
</ul></li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-18"></span>
<img src="pics/smith_a_fig2.jpg" alt="Coding of a continuous predictor." width="1522" />
<p class="caption">
Figure 3.4: Coding of a continuous predictor.
</p>
</div>
<ul>
<li><p>Notice the <span style=" font-weight: bold;    color: #E8793A !important;">positive hump</span> in the <span class="math inline">\(\beta_2\)</span> waveform peaking at 300 ms.</p>
<ul>
<li>This indicates an increased positivity for high-expectancy words …</li>
<li>… which is the same as a negativity to low-expectancy words.</li>
</ul></li>
<li><p>This is effect is reflected in the predicted ERPs: It is <span style=" font-weight: bold;    color: #6BBFA6 !important;">more positive for expected words</span> (<span class="math inline">\(x_{2i} = 1\)</span>) <span style=" font-weight: bold;    color: #375B42 !important;">than for unexpected</span> words (<span class="math inline">\(x_{2i} = 0\)</span>).</p></li>
<li><p>These predictors are pretty similar to the ones we used for treatment coding before:</p>
<ul>
<li>We have an <span style=" font-weight: bold;    color: #6BBFA6 !important;">intercept term</span> <span class="math inline">\(x_{1i}\)</span> whose corresponding parameter <span class="math inline">\(\beta_1\)</span> gives us a kind of <span style=" font-weight: bold;    color: #6BBFA6 !important;">baseline</span>.</li>
<li>We have an additional term <span class="math inline">\(x_{2i}\)</span> which codes for <span style=" font-weight: bold;    color: #6BBFA6 !important;">deviations from this baseline</span>.</li>
</ul></li>
<li><p>The difference here is:</p>
<ul>
<li>In the treatment coding before, the second parameter <span class="math inline">\(\beta_2\)</span> measures the difference in scalp voltage <span style=" font-weight: bold;    color: #6BBFA6 !important;">between <em>a</em> and <em>an</em> stimuli</span>, and has units of <span class="math inline">\(\mu V\)</span>.</li>
<li>Here, <span class="math inline">\(\beta_2\)</span> measures the difference in scalp voltage <span style=" font-weight: bold;    color: #6BBFA6 !important;">corresponding to any one unit change in expectancy</span>, and has units of <span class="math inline">\(\mu V\)</span> per unit change in expectancy.</li>
</ul></li>
<li><p>So it’s pretty much the same as the difference ERP before, only that we allow stimuli to fall in between the two extremes.</p></li>
</ul>
</div>
<div id="combining-categorical-and-continuous-predictors" class="section level3">
<h3><span class="header-section-number">3.2.6</span> Combining categorical and continuous predictors</h3>
<p>We combine the categorical and continuous predictors from before:</p>
<p><span class="math display">\[
\boldsymbol{x}_{1 i}=\mathbf{1} \quad \boldsymbol{x}_{2 i}=\left\{\begin{array}{ll}{\mathbf{0},} &amp; {\text { if stimulus } i \text { is a}} \\ {\mathbf{1},} &amp; {\text { if stimulus } \boldsymbol{i} \text { is an }}\end{array} \quad \boldsymbol{x}_{3 i}=\text { word expectancy on trial } i\right.
\]</span></p>
<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<img src="pics/smith_a_fig3_full.jpg" alt="Coding of a continuous and a categorical predictor combined." width="100%" />
<p class="caption">
Figure 3.5: Coding of a continuous and a categorical predictor combined.
</p>
</div>
<p>The resulting <span class="math inline">\(\beta\)</span>s have the same interpretation as they did in the previous examples:</p>
<ul>
<li><span class="math inline">\(\beta_1\)</span> gives the predicted ERP for <em>a</em> trials with <span style=" font-weight: bold;    color: #6BBFA6 !important;">zero measured expectancy</span>.</li>
<li><span class="math inline">\(\beta_2\)</span> gives the difference between <em>an</em> trials and <em>a</em> trials.
<ul>
<li>This model assumes that this is the same for all expectancies.</li>
</ul></li>
<li><span class="math inline">\(\beta_3\)</span> gives the change in the predicted ERP for each unit change in expectancy.</li>
</ul>
<div id="with-an-interaction" class="section level4">
<h4><span class="header-section-number">3.2.6.1</span> With an interaction</h4>
<p>For that, we would include <span class="math inline">\(x_{4i} = x_{2i} \times x_{3i}\)</span>, and if in that case …</p>
<ul>
<li><span class="math inline">\(\beta_3\)</span> denotes the expectancy effect for <em>a</em> stimuli.</li>
<li><span class="math inline">\(\beta_4\)</span> denotes the <span style=" font-weight: bold;    color: #6BBFA6 !important;">difference between</span> the expectancy effect for <em>a</em> stimuli and the expectancy effect for <em>an</em> stimuli.</li>
</ul>
</div>
<div id="more-complex-cases" class="section level4">
<h4><span class="header-section-number">3.2.6.2</span> More complex cases</h4>
<ul>
<li>Maybe some events have <span style=" font-weight: bold;    color: #6BBFA6 !important;">more or different predictors</span> associated with them than others
– E.g., in a go/no-go paradigm, go trials will have an associated response time, while no-go trials will not.</li>
<li>Two options:
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Divide trials into two separate bins and perform separate analyses on each bin.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Use the “zero trick” again: Simply <span style=" font-weight: bold;    color: #6BBFA6 !important;">include both sets of predictors into a single model</span>, with the rule that <span style=" font-weight: bold;    color: #6BBFA6 !important;">whenever we have a trial where a predictor doesn’t apply then we set it to zero</span>:</li>
</ol></li>
</ul></li>
</ul>
<p><span class="math display">\[
x_{1 i}=\left\{\begin{array}{ll}{1,} &amp; {\text { on } \mathrm{no}-\mathrm{go} \text { trials }} \\ {0,} &amp; {\text { on go trials }}\end{array}\right.
\]</span></p>
<p><span class="math display">\[
x_{2 i}=\left\{\begin{array}{ll}{0,} &amp; {\text { on } \mathrm{no}-\mathrm{go} \text { trials }} \\ {1,} &amp; {\text { on go trials }}\end{array}\right.
\]</span></p>
<p><span class="math display">\[
x_{3 i}=\left\{\begin{array}{cc}{0,} &amp; {\text { on no-go trials }} \\ {\mathrm{RT}_{i},} &amp; {\text { on go trials }}\end{array}\right.
\]</span></p>
<p>This way, the predictor’s (<span class="math inline">\(x_{3i}\)</span>) <span class="math inline">\(\beta\)</span> value will have no effect on our model’s prediction for no-go trials.</p>
</div>
</div>
</div>
<div id="collinearity" class="section level2">
<h2><span class="header-section-number">3.3</span> Collinearity</h2>
<ul>
<li>Problem for models with <span style=" font-weight: bold;    color: #6BBFA6 !important;">large numbers of predictors</span> and/or those with <span style=" font-weight: bold;    color: #6BBFA6 !important;">partially confounded covariates</span>.</li>
</ul>
<div id="definition" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Definition</h3>
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Collinearity</span> is typically used to refer to <span style=" font-weight: bold;    color: #6BBFA6 !important;">two very distinct phenomena</span>.
<ul>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Perfect collinearity</span> is when predictors are <span style=" font-weight: bold;    color: #6BBFA6 !important;">perfectly redundant</span> with each other.
– They make exactly the <span style=" font-weight: bold;    color: #6BBFA6 !important;">same predictions in different ways</span>.
– Discussed above as a motivation for treatment coding.
<ul>
<li>Straightforward effects, straightforward to avoid.</li>
</ul></li>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Partial collinearity</span> (aka <span style=" font-weight: bold;    color: #6BBFA6 !important;">partial confounding</span> or <span style=" font-weight: bold;    color: #6BBFA6 !important;">non-orthogonality</span>) refers to the situation where some predictors are <span style=" font-weight: bold;    color: #6BBFA6 !important;">correlated, but not identical</span>.
<ul>
<li>Sometimes the word collinearity is reserved for “severe” cases, where the degree of correlation among predictors exceeds some threshold.</li>
<li>Since such thresholds are arbitrary; the discussion applies equally to both mild and severe cases.</li>
</ul></li>
</ul></li>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Partial collinearity</span> does not violate any assumptions of the least-squares regression.
<ul>
<li>However: There two things we need to watch out for: whether we have <span style=" font-weight: bold;    color: #6BBFA6 !important;">enough predictors</span>, and whether we have <span style=" font-weight: bold;    color: #6BBFA6 !important;">enough data</span>.</li>
</ul></li>
</ul>
</div>
<div id="enough-predictors" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Enough predictors</h3>
<ul>
<li>Problems can arise from leaving out relevant predictors:</li>
<li>Figs. 3.4 and 3.5 both show estimates of the slope rERP <span class="math inline">\(\beta\)</span> for expectancy.
<ul>
<li>Fig. 3.5 includes the <em>a</em>/<em>an</em> predictor.</li>
<li>Fig. 3.4 leaves out the <em>a</em>/<em>an</em> predictor.</li>
</ul></li>
<li>In this case, the estimates are nearly identical.
<ul>
<li>Typical of <span style=" font-weight: bold;    color: #6BBFA6 !important;">(near-)orthogonal</span> experiments: This one was carefully designed to ensure that the <em>a</em> stimuli and <em>an</em> stimuli were <span style=" font-weight: bold;    color: #6BBFA6 !important;">balanced</span> with respect to expectancy.</li>
<li>In orthogonal designs, leaving out a covariate doesn’t change our estimates for the remaining covariates.</li>
</ul></li>
<li>Imagine a version of this experiment where the <em>an</em> stimuli had, on average, a lower expectancy than the <em>a</em> stimuli.
<ul>
<li>Then the best way for the <span style=" font-weight: bold;    color: #6BBFA6 !important;">expectancy-only</span> model to capture the patterns in the data would be for it to attribute some of the categorical <em>an</em> effect to expectancy.</li>
<li>The analysis becomes <span style=" font-weight: bold;    color: #6BBFA6 !important;">inconsistent</span>: It doesn’t matter <span style=" font-weight: bold;    color: #6BBFA6 !important;">how much data we have</span>, the results will always be misleading.</li>
</ul></li>
<li>But in the <span style=" font-weight: bold;    color: #E8793A !important;">combined model</span>, any patterns associated with the word <em>an</em> can be attributed to it directly.
<ul>
<li>“Because the least-squares fitting process selects whichever <span class="math inline">\(\beta\)</span> coefficients are best at working together to explain the patterns in the data, and ultimately, the covariates that are actually responsible for a pattern are always the best way to explain it. As long as we include those covariates in our model, and have enough data, then regression will work out the correct waveforms even if our predictors are confounded.”</li>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Caution:</span> I doubt that this is true in any case and this almost sounds like a regression can uncover causal effects, which it can’t?</li>
</ul></li>
</ul>
</div>
<div id="enough-data" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Enough data</h3>
<ul>
<li>Collinearity also affects the <span style=" font-weight: bold;    color: #6BBFA6 !important;">noise in our estimates</span> and <span style=" font-weight: bold;    color: #6BBFA6 !important;">how much data we need</span>.</li>
<li>The key to understanding this is to examine <span style=" font-weight: bold;    color: #E8793A !important;">variance inflation factors</span> (VIFs).
<ul>
<li>Each VIF measures <span style=" font-weight: bold;    color: #6BBFA6 !important;">how the sampling variance of one estimated</span> <span class="math inline">\(\beta\)</span> <span style=" font-weight: bold;    color: #6BBFA6 !important;">coefficient is affected by the presence of collinearity</span>, and is equal to <span class="math inline">\(1/(1 − R^2)\)</span>.</li>
<li>The <span class="math inline">\(R^2\)</span> here is not taken from the model we actually want to fit, but instead measures how much of the <span style=" font-weight: bold;    color: #6BBFA6 !important;">variance in this predictor</span> can be explained by the <span style=" font-weight: bold;    color: #6BBFA6 !important;">other predictors</span> in our model.
<ul>
<li>Example: If we have a model with just two predictors, and the correlation between these two predictors is <span class="math inline">\(r = 0.5\)</span>, then the VIFs will be <span class="math inline">\(1/(1 − 0.5^2) \approx 1.33\)</span>.</li>
<li>That means: If we know that normally we would need 60 trials to get an acceptably accurate estimate of the rERP for a single predictor in an orthogonal design, then we need 60 <span class="math inline">\(\times\)</span> 1.33 = 80 trials to get equally accurate estimates of the rERPs to these two predictors <span class="citation">(O’brien, 2007)</span>.</li>
<li>Put another way: each trial in this design is worth three-quarters of a trial in a fully orthogonal design, at least with respect to these predictors.</li>
<li>It is generally wise to compute the VIF while planning an experiment rather than waiting until the analysis stage.</li>
</ul></li>
</ul></li>
</ul>
<p>Implications from the VIF formula:</p>
<ul>
<li>If we add <span style=" font-weight: bold;    color: #6BBFA6 !important;">additional uncorrelated predictors</span> to our model, then this has absolutely <span style=" font-weight: bold;    color: #6BBFA6 !important;">no effect on the noisiness</span> of our estimates.
<ul>
<li>Any predictor in an orthogonal design has a VIF of 1, which means that so long as our covariates are uncorrelated, we can estimate a dozen parameters just as well as we can estimate one of them, from the same amount of data – though in practice we may be hard-pressed to find a dozen uncorrelated predictors.</li>
<li><span style=" font-weight: bold;    color: #E8793A !important;">Collinearity is not contagious:</span> If we have two predictors which are correlated with each other but both are uncorrelated with a third, then we will need more data to estimate the effect of the first two predictors, but their presence in the model will not affect estimates for the third predictor.</li>
</ul></li>
<li><span style=" font-weight: bold;    color: #E8793A !important;">VIFs &gt; 1 travel in packs:</span> Correlation always goes (at least) two ways.
<ul>
<li>Whenever one predictor has a VIF &gt; 1, others will as well.</li>
</ul></li>
</ul>
<p>"This is closely related to the underlying cause of variance inflation. Recall how when we had two <span style=" font-weight: bold;    color: #6BBFA6 !important;">identical predictors</span>, the model might know that to match the data it needed <span class="math inline">\(\beta_1 + \beta_2 = 1\)</span>, but could not distinguish between an option like <span class="math inline">\(\beta_1 = 0, \beta_2 = 1\)</span> and one like <span class="math inline">\(\beta_1 = 100, \beta_2 = -99\)</span>. When our predictors are not identical, but <span style=" font-weight: bold;    color: #6BBFA6 !important;">merely similar</span>, then our model can distinguish these cases in principle – but given <span style=" font-weight: bold;    color: #6BBFA6 !important;">finite data</span>, it may struggle; these two options will make very similar predictions, and depending on the noise, setting <span class="math inline">\(\beta_1 = 100, \beta_2 = -99\)</span> may seem unreasonably attractive.</p>
<p>Technically, this struggle manifests as an <span style=" font-weight: bold;    color: #6BBFA6 !important;">increased sampling variance</span> for all the involved <span class="math inline">\(\beta\)</span>s. This is the extra sampling variance measured by the VIFs, and it manifests in a distinctive way. In orthogonal designs, the background noise corrupts our estimates for each <span class="math inline">\(\beta\)</span> independently: <span class="math inline">\(\beta_1\)</span> might be erroneously high while <span class="math inline">\(\beta_2\)</span> is erroneously low, or vice-versa, or both might be erroneously high or both might be erroneously low – there’s no correlation between the noise in <span class="math inline">\(\beta_1\)</span> and the noise in <span class="math inline">\(\beta_2\)</span>.</p>
<p>Designs containing collinearity are different: the same noise affects multiple <span class="math inline">\(\beta\)</span> values simultaneously, and can produce <span style=" font-weight: bold;    color: #6BBFA6 !important;">strange, structured behaviour</span>, where – for example, in the most common manifestation – every time <span class="math inline">\(\beta_1\)</span> is estimated erroneously high, <span class="math inline">\(\beta_2\)</span> will be estimated erroneously low, and vice-versa. And, because <span style=" font-weight: bold;    color: #6BBFA6 !important;">collinearity effectively increases the amount of noise in our estimates</span>, if we don’t have enough data then we should expect to sometimes see <span class="math inline">\(\beta_1\)</span> estimated <em>very</em> high (<span class="math inline">\(\beta_1 = 100\)</span>), while <span class="math inline">\(\beta_2\)</span> is estimated very low (<span class="math inline">\(\beta_2 = -99\)</span>). This is <span style=" font-weight: bold;    color: #E8793A !important;">fully expected within the theory of linear regression</span>, and accounted for in ordinary statistical tests. If one participant’s estimates have <span class="math inline">\(\beta_1 = 100\)</span>, <span class="math inline">\(\beta_2 = -99\)</span>, then the next will have <span class="math inline">\(\beta_1 = -99\)</span>, <span class="math inline">\(\beta_2 = 100\)</span>, and our statistics will correctly conclude that these estimates are too noisy to indicate anything real. But it’s important to know about this behavior in order to understand what’s happening when we see it.</p>
</div>
<div id="simplified-models-as-solution" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Simplified models as solution?</h3>
<p>When collinearity is too high, and we cannot gather enough data to achieve tight estimates of our <span class="math inline">\(\beta\)</span> parameters, is it a good idea to toss out some predictors and simplify our model? Spoiler: no.</p>
<div id="collinearcontrols" class="section level4">
<h4><span class="header-section-number">3.3.4.1</span> Case of collinear controls</h4>
<p>If the predictors with high VIFs are included as <span style=" font-weight: bold;    color: #E8793A !important;">controls</span> in our model, but are not actually the target of our scientific questions (i.e. we are not going to interpret them anyway), then these control factors will be very noisy, but <span style=" font-weight: bold;    color: #6BBFA6 !important;">collinearity is not a problem</span>.</p>
<ul>
<li>If our controls are strongly correlated with each other, but only weakly correlated with the predictors that are the actual targets of our analysis, then we can leave in all our control predictors without worrying that their instability will infect our other <span class="math inline">\(\beta\)</span>s.</li>
<li>While the <span class="math inline">\(\beta\)</span>s estimated for these predictors will be noisy and uninterpretable, their presence will provide a <span style=" font-weight: bold;    color: #6BBFA6 !important;">valuable control</span> on the <span class="math inline">\(\beta\)</span>s we do care about, reducing the chance of finding inconsistent, spurious results due to confounding.</li>
<li><span style=" font-weight: bold;    color: #E8793A !important;">NOTE:</span> Controlling for a predictor is not always advisable, though. <span style=" font-weight: bold;    color: #E8793A !important;">TO DO:</span> Again, have a look at the difference between <span style=" font-weight: bold;    color: #E8793A !important;">controlling for</span> and <span style=" font-weight: bold;    color: #E8793A !important;">conditioning on</span>, what a regression is doing in that context and what to watch out for.</li>
</ul>
</div>
<div id="case-of-collinear-targets" class="section level4">
<h4><span class="header-section-number">3.3.4.2</span> Case of collinear targets</h4>
<p>When the <span style=" font-weight: bold;    color: #E8793A !important;">target predictors</span> have high VIFs, removing controls collinear to our targets would give us more statistical power, i.e., our VIFs would go down and our effective data set size would increase.</p>
<ul>
<li>But: If we remove the wrong predictors, then even an infinite amount of power will only let us more precisely produce the wrong answer.</li>
<li>We lose the ability to distinguish between the controls and the covariates we care about.</li>
</ul>
<p><span style=" font-weight: bold;    color: #E8793A !important;">Discuss, though:</span> “When we remove a control variable, what we’re assuming is that we know a priori that it has no effect; but if we know that, then we why did we include it in the first place?”</p>
<ul>
<li><p>It’s slightly more complicated, isn’t it? Because other variables could have taken care of it, controlling for that variable might introduce bias due to other variables …</p></li>
<li><p><span style=" font-weight: bold;    color: #E8793A !important;">TO DO:</span> Go through conditons when to control for a variable or not; probably with <a href="https://www.amazon.de/Book-Why-Science-Cause-Effect/dp/046509760X">Pearl’s book</a>.</p></li>
<li><p>Suppose we have two correlated variables and want to interpete their effects. If both come out significant when entered alone, but neither is significant when entered together, then it means that <span style=" font-weight: bold;    color: #6BBFA6 !important;">at least one of these predictors matters</span>, but more data are needed to distinguish their effects.</p>
<ul>
<li>E.g. stepwise regression will pick one of the predictors at random, without a warning <span class="math inline">\(\to\)</span> This does not provide any reliable evidence that the predictor chosen is more important than the other</li>
<li>Useful for e.g. developing medical screening models where the goal is to achieve high predictive accuracy while measuring an economical set of variables, and it doesn’t matter whether the items that end up being measured are the most direct reflections of the underlying process - but it makes them inappropriate for scientific research.</li>
<li>You could average redundant measures of the same construct together to reduce noise, but for e.g. the <a href="smith-and-kutas-2015a.html#collinearcontrols">case of collinear controls</a>, the authors rather strongly advised against that.</li>
</ul></li>
<li><p>Different kinds of coding may reduce noise (e.g. treatment coding will give difference predictions that are less noisy), but this does not alter the overall model fit or the model’s predictions.</p></li>
<li><p>“One kind of recoding that in particular is <span style=" font-weight: bold;    color: #6BBFA6 !important;">hard to justify is the use of PCA</span> for the sole purpose of orthogonalizing predictors before entering them into regression. Like other kinds of recoding, this has no effect on model fit overall; the only motivation for doing it is if we want to interpret our individual <span class="math inline">\(\beta\)</span>s, but are prevented by collinearity. Orthogonalization replaces our original <span class="math inline">\(\beta\)</span>s with a new set that have reduced VIFs and thus are less noisy; but in the process it always makes individual <span class="math inline">\(\beta\)</span>s less interpretable, by replacing them with strange and arbitrary linear combinations of our original <span class="math inline">\(\beta\)</span>s. The procedure is therefore self-defeating.”</p></li>
</ul>
<p><span style=" font-weight: bold;    color: #E8793A !important;">I profoundly disagree:</span> “The best rule of thumb seems to be, enter all the predictors that may be relevant, report the full list of predictors that were entered along with details of any transformations and coding schemes, and then test and report whichever waveforms and contrasts are of scientific interest.”</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fromer-et-al-2018.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="smith-and-kutas-2015b.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": 2,
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
